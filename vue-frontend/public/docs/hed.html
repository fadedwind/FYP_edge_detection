<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width,initial-scale=1">
  <title>HED — Holistically-Nested Edge Detection 论文总结与复现指南</title>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/water.css@2/out/water.css">
  <!-- MathJax 加载，用于渲染 LaTeX 公式 -->
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <style>
    body{max-width:980px;margin:28px auto;padding:0 18px;font-family:system-ui,Segoe UI,Helvetica,Arial}
    h1,h2,h3{margin-top:1.1rem}
    pre{background:#f6f8fa;padding:10px;overflow:auto}
    table{border-collapse:collapse;width:100%}
    table th, table td{border:1px solid #ddd;padding:8px;text-align:left}
    .small{font-size:0.9rem;color:#555}
  </style>
</head>
<body>
  <h1>HED — Holistically-Nested Edge Detection</h1>
  <p class="small">基于论文《Holistically-Nested Edge Detection》，总结于：2025-11-11（整理者）。本文档包含核心思想、数学公式、复现思路与实现细节，适合作为复现/工程实现参考。</p>

  <h2>一、论文核心贡献（概要）</h2>
  <ul>
    <li>提出 HED（Holistically-Nested Edge Detection）：一种端到端的深度学习边缘检测方法，直接从图像到像素级边缘概率图。</li>
    <li>多尺度多层级特征融合：在深层卷积网络的多个中间层添加 <em>侧输出层（side outputs）</em>，提取不同感受野/尺度的边缘线索，并通过加权融合层融合这些侧输出。</li>
    <li>深度监督（Deep Supervision）：给每个侧输出加独立的损失函数，促使浅层和深层都学到有意义的边缘响应，从而提升性能。</li>
    <li>在 BSDS500、NYUD 等数据集上取得当时最优的 F-score，且推理速度优于同时期的其它 CNN 方法。</li>
  </ul>

  <h2>二、符号与问题定义</h2>
  <p>训练集记为 \(S=\{(X_n, Y_n)\}\)。对第 \(n\) 张图，令：</p>
  <ul>
    <li>图像：\(X_n=\{x_j^{(n)}\}_{j=1}^{|X|}\)</li>
    <li>边缘标注（每像素二值）：\(Y_n=\{y_j^{(n)}\},\; y_j\in\{0,1\}\)</li>
    <li>网络参数：主干参数为 \(W\)，所有侧输出层的分类器参数集合为 \(w=(w^{(1)},\dots,w^{(M)})\)，融合层的权重为 \(h=(h_1,\dots,h_M)\)。</li>
  </ul>

  <h2>三、损失函数与训练目标（公式）</h2>
  <p>HED 使用 <strong>侧输出损失 + 融合层损失</strong> 的组合来训练网络：</p>

  <h3>1. 侧输出层损失</h3>
  <p>侧输出的总体损失为：</p>
  \[
    L_{side}(W,w)=\sum_{m=1}^M \alpha_m\,\ell_{side}^{(m)}(W,w^{(m)})
  \]
  <p>其中单个侧输出的像素级别加权交叉熵写为：</p>
  \[
    \ell_{side}^{(m)}(W,w^{(m)}) = -\beta\sum_{j\in Y_{+}}\log\Pr(y_j=1\mid X;W,w^{(m)}) - (1-\beta)\sum_{j\in Y_{-}}\log\Pr(y_j=0\mid X;W,w^{(m)})
  \]
  <p>这里 \(Y_{+}\) 为正样本像素索引集合（边缘像素），\(Y_{-}\) 为负样本集合（非边缘像素），并且平衡因子定义为：</p>
  \[
    \beta = \frac{|Y_{-}|}{|Y|},\quad 1-\beta = \frac{|Y_{+}|}{|Y|}
  \]
  <p>概率用 sigmoid 建模：\(\Pr(y_j=1\mid X)=\sigma(a_j^{(m)})\)，其中 \(a_j^{(m)}\) 是侧输出在像素 \(j\) 的激活。</p>

  <h3>2. 融合层损失</h3>
  <p>融合层将所有侧输出的激活线性加权，然后经过 sigmoid 得到融合输出 \(\widehat{Y}^{fuse}\)：</p>
  \[
    \widehat{Y}^{fuse}=\sigma\Big(\sum_{m=1}^M h_m\,\widehat{A}^{side(m)}\Big)
  \]
  <p>融合层的损失通常直接使用像素级距离/交叉熵（本文以 Dist 表示）：</p>
  \[
    L_{fuse}(W,w,h)=\mathrm{Dist}(Y,\widehat{Y}^{fuse})
  \]

  <h3>3. 总损失</h3>
  <p>训练目标为最小化侧输出损失与融合损失之和：</p>
  \[
    (W,w,h)^* = \arg\min_{W,w,h} \big( L_{side}(W,w) + L_{fuse}(W,w,h) \big)
  \]

  <h2>四、复现思路（整体流程）</h2>

  <h3>1. 网络架构</h3>
  <ol>
    <li>主干：基于 VGG-16（去掉最后的全连接层以及 pool5，保留 conv 层）</li>
    <li>侧输出：在 conv1_2、conv2_2、conv3_3、conv4_3、conv5_3 后各添加一个侧输出（1x1 卷积 + sigmoid），得到 M=5 个侧输出。</li>
    <li>上采样：每个侧输出使用双线性上采样（或可学习反卷积）把输出上采样到原图大小。</li>
    <li>融合层：对上采样后的侧输出做线性加权求和（权重 \(h_m\)），再通过 sigmoid 得到最终融合概率图。</li>
  </ol>

  <h3>2. 训练细节</h3>
  <ul>
    <li>初始化：使用 ImageNet 预训练的 VGG-16 权重，侧输出和融合层随机初始化。</li>
    <li>数据增强：旋转（16 个角度）+ 翻转 → 共 32 倍；另外做缩放（50%、100%、150%）。</li>
    <li>共识采样：只把至少 k（论文典型为 3）位标注者同意的像素当作正样本，以降低噪声标注影响。</li>
    <li>超参数参考：batch size=10，初始 lr=1e-6（训练 5000 次后降到 1e-7），动量=0.9，weight decay=0.0002；侧输出权重 \(\alpha_m=1\)，融合权重初始化为均匀 \(h_m=1/5\)。</li>
  </ul>

  <h3>3. 损失与优化</h3>
  <p>对每个侧输出使用像素级加权交叉熵（上文给出），融合输出也使用像素级损失。整体做联合反向传播（deep supervision 提供中间梯度约束）。优化器可选 SGD（momentum）或 Adam（但论文使用 Caffe 的 SGD 设置）。</p>

  <h2>五、关键实现细节与工程提示</h2>
  <ol>
    <li>侧输出实现：侧输出通常为 1x1 卷积将通道数降为 1（logit），然后上采样到原图尺寸并使用 sigmoid 变为概率。</li>
    <li>上采样方式：双线性插值足够且简单；若使用转置卷积（learned upsampling），需谨慎初始化以避免 checkerboard artifact。</li>
    <li>平衡正负样本：慎重实现 \(\beta\) 的计算（对于每张图或每个 mini-batch），防止数值不稳定。</li>
    <li>深度监督权重：论文对每层的 \(\alpha_m\) 取 1，但工程实践可把浅层或深层加权调整为更优表现。</li>
    <li>边缘稀疏性：边缘像素远少于非边缘像素，使用类别平衡交叉熵是关键；也可尝试 Focal loss 做进一步的样本难易度加权。</li>
    <li>训练监控：记录每个侧输出与融合输出在验证集上的 F-score（ODS/OIS）以观察深度监督的实际贡献。</li>
  </ol>

  <h2>六、性能表现（论文中报告的结果，供参考）</h2>
  <table>
    <thead>
      <tr><th>方法</th><th>ODS</th><th>OIS</th><th>AP</th><th>FPS(约，GPU)</th></tr>
    </thead>
    <tbody>
      <tr><td>HED（融合+深度监督）</td><td>0.790</td><td>0.808</td><td>0.811</td><td>2.5</td></tr>
      <tr><td>HED（仅融合）</td><td>0.785</td><td>0.801</td><td>0.730</td><td>-</td></tr>
      <tr><td>HED（后融合）</td><td>0.788</td><td>0.808</td><td>0.840</td><td>-</td></tr>
    </tbody>
  </table>

  <h2>七、复现建议与工具链</h2>
  <ul>
    <li>首选框架：Caffe（原作者），若使用现代框架则优先选择 PyTorch（生态、调试与部署更方便）。</li>
    <li>参考实现：作者公开实现 <a href="https://github.com/s9xie/hed" target="_blank">s9xie/hed (GitHub)</a>，包含训练/测试脚本与预训练模型。</li>
    <li>数据准备：BSDS500 的标注需要做共识采样；NYUD 数据集需同时使用 RGB+HHA（若复现实验需留意）。</li>
    <li>度量工具：使用像素级的 Precision/Recall/F1，报告 ODS（dataset-level）与 OIS（image-level）；可用 BSDS 官方评测脚本做对齐评估。</li>
  </ul>

  <h2>八、常见问题与调试提示</h2>
  <ol>
    <li>边缘很 "厚"：确认是否在侧输出后使用了非极大值抑制 (NMS)。HED 输出通常是概率图，若需要细化边缘可在推理后做 NMS/细化。</li>
    <li>训练不收敛或侧输出失效：检查是否正确加载 VGG 预训练权重（层名/shape 对不上会导致训练从头开始）；验证各侧输出的损失是否在下降。</li>
    <li>内存或速度问题：使用较小的 batch 或图像短边缩放；推理时可以只使用融合层输出以降低 I/O。</li>
    <li>标签不一致：强烈建议使用共识采样来筛掉弱标注；否则正负样本噪声会影响收敛与最终 F-score。</li>
  </ol>

  <h2>九、参考文献与链接</h2>
  <ol>
    <li>Xie S., Tu Z., "Holistically-Nested Edge Detection", ICCV 2015. <a href="https://arxiv.org/abs/1504.06375" target="_blank">arXiv:1504.06375</a></li>
    <li>作者开源实现：<a href="https://github.com/s9xie/hed" target="_blank">https://github.com/s9xie/hed</a></li>
    <li>BSDS500 数据集：<a href="https://www2.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/BSR/" target="_blank">BSR / BSDS500</a></li>
  </ol>

  <hr>
  <p class="small">备注：本文档侧重于工程复现与公式整理，若需要我可基于 PyTorch 帮你生成一份最小可运行的 HED 复现代码（含训练/验证脚本与 Dockerfile），或把该页面并入项目的文档首页。</p>
</body>
</html>
